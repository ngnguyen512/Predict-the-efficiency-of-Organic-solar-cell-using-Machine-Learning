{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b7b3b619",
   "metadata": {},
   "source": [
    "## Importing the last file that has the functions we need to apply the model and for plotting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "05f98a0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is import from the file: MLChem.Final-Project-Anjan.ipynb\n"
     ]
    }
   ],
   "source": [
    "%run MLChem.Final-Project-Anjan.ipynb\n",
    "print(\"This is import from the file: MLChem.Final-Project-Anjan.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "414df9be",
   "metadata": {},
   "source": [
    "## Steps to clean and test the model!\n",
    "1. Read the first and second, and third file and remove descriptors that are not relevant\n",
    "2. Merge all these new files downloaded from the mordred into one!. (These are the additional descripors we use)\n",
    "3. Again merge the file with the original sahu file and test the model\n",
    "4. Test the feature importances! or check the r value individually!\n",
    "5. Test model with different descriptors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ebe2378",
   "metadata": {},
   "source": [
    "### Step1 & Step 2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "602330a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the first file!\n",
    "data_a = pd.read_csv('New_descriptor.csv')\n",
    "data_a.head()\n",
    "\n",
    "# get only the descriptors you want!\n",
    "data_one = data_a[['fMF', 'nHBAcc','apol','bpol', 'Vabc', 'MW', 'AMW']]\n",
    "data_one.insert(loc=0, column='sn', value=np.arange(len(data_one)))\n",
    "# data_one.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7085f0b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the second file!\n",
    "read_b = pd.read_csv('new_descriptors_b.csv')\n",
    "\n",
    "# get only the descriptors you want!\n",
    "data_two = read_b[['ABC','ABCGG','nAromAtom','nAromBond']]\n",
    "data_two.insert(loc=0, column='sn', value=np.arange(len(data_two)))\n",
    "# data_two.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a9a16712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading the third file!\n",
    "read_c = pd.read_csv('Mordred2.csv')\n",
    "read_c.head()\n",
    "# read_c.columns\n",
    "# get only the descriptors you want!\n",
    "data_three = read_c[['C1SP2', 'C2SP2', 'C3SP2','C1SP3','HybRatio', 'FCSP3']]\n",
    "data_three.insert(loc=0, column='sn', value=np.arange(len(data_three)))\n",
    "\n",
    "# data_three.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "614486fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge between one and two \n",
    "mordred_descripors_a = pd.merge(data_one,data_two,on='sn')\n",
    "# mordred_descripors_a.head()\n",
    "\n",
    "# merge between two and three\n",
    "mordred_descripors_final = pd.merge(mordred_descripors_a,data_three,on='sn')\n",
    "# mordred_descripors_final.head()\n",
    "# created a new mordred_descriptors file that has all the descripors we will use in addition to the sahu descripors!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4304fd1",
   "metadata": {},
   "source": [
    "### Steps 3 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e574e39",
   "metadata": {},
   "source": [
    "**Clean the original code by sahu to prepare it for merging.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "984864cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# drop the defective rows in the original datasets!\n",
    "\n",
    "def orig_droped_df(ori_file,row_drop_g1):\n",
    "    ori_file= pd.read_csv(ori_file)\n",
    "    sahu_new_descriptor = ori_file.drop(row_drop_g1)\n",
    "\n",
    "    # adding the sn so that we can merge the table using same key\n",
    "    sahu_new_descriptor.insert(loc=0, column='sn', value=np.arange(len(sahu_new_descriptor)))\n",
    "\n",
    "    # let's drop some of the columns in original file because we don't need them!\n",
    "    sahu_new_descriptor = sahu_new_descriptor.drop(columns=['#Sno.'])\n",
    "    \n",
    "    return sahu_new_descriptor\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583408de",
   "metadata": {},
   "source": [
    "**Create a new merged file identical to the sahu datasets!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fbe883f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merged_final():\n",
    "    # get these two files and merge it to one! (ori_file is original datasets by sahu and new_file is downloaded one)\n",
    "    ori_file = 'Sahu_original_dataset.csv'\n",
    "    \n",
    "    # this below file is created during step 1 and 2.\n",
    "    new_file = pd.read_csv('mordred_descriptors.csv')\n",
    "    \n",
    "    # drop_row and columns\n",
    "    row_drop_ori = [6,17,18,22,28,40,58,66,116,156,172,174,184,269,270,271,273,277,278] # drop rows of orginal file\n",
    "\n",
    "    # change the new file to make it identical file to the original file\n",
    "    # new_file_make = make_new_dnlod_file(new_file) # already done the sn addition\n",
    "    \n",
    "    # Drop undwated columns from the new file since we won't need all descriptor!\n",
    "    # new_file_remake = drop_coln_new_dnlod_file(new_file_make, drop_coln_nf)\n",
    "    \n",
    "    # this removes the unwated rows in original file \n",
    "    get_orig_file = orig_droped_df(ori_file,row_drop_ori)\n",
    "    \n",
    "    # now we have two same file with same rows, we will merge them, 'sn' that we adde is a common key to both file!\n",
    "    new_merged_df = pd.merge(get_orig_file,new_file,on='sn')\n",
    "    \n",
    "    return new_merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50daadfb",
   "metadata": {},
   "source": [
    "### Step 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e216a4",
   "metadata": {},
   "source": [
    "#### File 'return_descriptor_a.csv' is a merged file that has sahu original datasets and all th descriptors we get from the three files downloaded from the mordred.web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6da89eed",
   "metadata": {},
   "outputs": [],
   "source": [
    "file = pd.read_csv('return_descriptor_a.csv')\n",
    "file2 = file.drop(columns=['Unnamed: 0'])\n",
    "\n",
    "# replace the nan value with 0!\n",
    "# file3 is the df with replaced nan value.\n",
    "file3 = file2.replace(np.nan,0)\n",
    "# len(file3.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9781e",
   "metadata": {},
   "source": [
    "#### A function to get list of descripors which returns higher r-square value when tested against PCE!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbbc5406",
   "metadata": {},
   "source": [
    "### Step 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791de2e5",
   "metadata": {},
   "source": [
    "### Writing a function to check the descriptor with high feature importance and return their name, helping decide what descriptor to use!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c76c3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the pearson test:\n",
    "# ls_person_test = ['polarizability','delLA','delLD','N_atom','delHD','E_bind','DL-AL','fMF','nHBAcc',\n",
    "#  'apol','MW','AMW','ABC','ABCGG','nAromAtom','nAromBond','C2SP2','HybRatio','FCSP3']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "cd490635",
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is baed on the r value of test higher than 0.30\n",
    "# d2 = file3[['PCE','polarizability','delLA','delLD','N_atom','delHD','E_bind','DL-AL','fMF','nHBAcc',\n",
    "#  'apol','MW','AMW','ABC','ABCGG','nAromAtom','nAromBond','C2SP2','HybRatio','FCSP3']]\n",
    "# d2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1479badf",
   "metadata": {},
   "source": [
    "### 5A."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25065fd0",
   "metadata": {},
   "source": [
    "**Based on feature importanhce from Sahu! we have these descripotors** \n",
    "\n",
    "['N_atom', 'lamda_h', 'delHD', 'E_bind', 'DL-AL']\n",
    "\n",
    "**Based on good r value of a descriptors against PCE, we have** \n",
    "\n",
    "#### p_train_value = \n",
    "['PCE','polarizability','delLD','Eg','lamda_h','DIP','AL-DH','delHD','E_bind','DL-AL','delGE','E_T1',\n",
    " 'fMF','apol','bpol','Vabc','MW','AMW','ABC','ABCGG']\n",
    " \n",
    "#### p_test_value = \n",
    "['polarizability','delLA','delLD','N_atom','delHD','E_bind','DL-AL','fMF','nHBAcc',\n",
    " 'apol','MW','AMW','ABC','ABCGG','nAromAtom','nAromBond','C2SP2','HybRatio','FCSP3']\n",
    " \n",
    "#### haveing 0.70 above and 0.30 above:\n",
    "ls_new = ['polarizability','delLD','delHD','E_bind','DL-AL','fMF','apol','MW','AMW','ABC','ABCGG']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb593472",
   "metadata": {},
   "source": [
    "### After knowing what descriptor to use in the model, we can select those descriptors and predict the model!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c383d7ec",
   "metadata": {},
   "source": [
    "### 1a. Testing the descriptors with test_r value more than 0.30 above average (out of entire descriptor in File3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "24a91e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1ab. Testing the descriptors with test_r value more than 0.30 (out of entire descriptor in File3)\n",
    "df_test_01=file3[['PCE','polarizability','delLA','delLD','N_atom','E_bind','DL-AL','fMF','nHBAcc','apol',\n",
    " 'MW','AMW','ABC','ABCGG','nAromAtom','nAromBond','C2SP2','HybRatio','FCSP3']]\n",
    "# df_test_01.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407e6800",
   "metadata": {},
   "source": [
    "## 1b. Testing the only PCE with the descriptor from the mordred.web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "7a530049",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe:\n",
    "df_test_02 = file3[['PCE','fMF','nHBAcc','apol','MW','AMW','ABC','ABCGG','nAromAtom','nAromBond',\n",
    "                    'C2SP2','HybRatio','FCSP3']]\n",
    "# df_test_02.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7526a7f7",
   "metadata": {},
   "source": [
    "### 1c. Feature importance from the sahu (>0.70 average ) and total descriptors and the mordered.web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b2e2ee18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datraframe:\n",
    "df_test_03 = file3[['PCE','N_atom', 'lamda_h', 'delHD', 'E_bind', 'DL-AL','fMF','nHBAcc','apol','MW','AMW',\n",
    "                    'ABC','ABCGG','nAromAtom','nAromBond','C2SP2','HybRatio','FCSP3']]\n",
    "# df_test_03.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202942c1",
   "metadata": {},
   "source": [
    "## Model Testing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d90917e",
   "metadata": {},
   "source": [
    "## Test Using RF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "e5439eb4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rf = RandomForestRegressor()\n",
    "model = [rf]\n",
    "\n",
    "# for file3\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, file3, 'PCE')\n",
    "\n",
    "# for 1a.\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, df_test_01, 'PCE')\n",
    "\n",
    "# for 1b.\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, df_test_02, 'PCE')\n",
    "\n",
    "\n",
    "# for 1c.\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, df_test_03, 'PCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "6b5e2863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for file3\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n",
    "\n",
    "\n",
    "# for 1a\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n",
    "\n",
    "# for 1b\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n",
    "\n",
    "# for 1c\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aca75d16",
   "metadata": {},
   "source": [
    "## Test using gradient boosting!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d0a48a9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "gb = GradientBoostingRegressor()\n",
    "model = [gb]\n",
    "\n",
    "# for file3\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, file3, 'PCE')\n",
    "\n",
    "# for 1a\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, df_test_01, 'PCE')\n",
    "\n",
    "# for  1b\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, df_test_02, 'PCE')\n",
    "\n",
    "# for  1c\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, df_test_03, 'PCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "000faf42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting the img: For file3 which includes every descriptors\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n",
    "\n",
    "# plotting for 1a.\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n",
    "\n",
    "# plotting for 1b.\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n",
    "\n",
    "# plotting for 1c.\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
