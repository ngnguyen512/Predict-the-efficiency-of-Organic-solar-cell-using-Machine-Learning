{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML full codes\n",
    "**Congratulations! You're almost at the finish line.**\n",
    "A typical machine learning task in chemistry consists of three steps:\n",
    "- Import and explore data\n",
    "- Split data, select inputs and labels, train and evaluate\n",
    "- Plot and Analyze data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import and Explore data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from matplotlib import pyplot as plt \n",
    "\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor,GradientBoostingClassifier\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "from sklearn.metrics import mean_squared_error as rmse\n",
    "from sklearn.metrics import mean_absolute_error as mae\n",
    "from scipy.stats import pearsonr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read sahu datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sahu = pd.read_csv(\"Sahu_original_dataset.csv\", encoding = 'cp1252', on_bad_lines = 'skip')\n",
    "# sahu.head()\n",
    "# print(len(sahu))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Firstly, we get additional descriptor we need from the mordred.web sahu.smi file. This group selection includes.**\n",
    "\n",
    "[hydrogenbond, Lipinski, polarizability (apop and bpol), Vandarwalls, Molecular Weight (atomic molecular wt.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the new descriptor data!\n",
    "\n",
    "new_file = pd.read_csv(\"New_descriptor.csv\", encoding = 'cp1252', on_bad_lines = 'skip')\n",
    "new_file.insert(loc=0, column='sn', value=np.arange(len(new_file)))\n",
    "\n",
    "# let's drop some of the columns because we don't need them!\n",
    "new_file = new_file.drop(columns=['name','AMW','GhoseFilter'])\n",
    "# new_file.head()\n",
    "# print(len(new_file)) # There are 261 molecules where as sahu has 281 molecules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sahu dataset has 281 molecules. However, the sahu.smi does not read all the 281 files, it only reads upto 261 molecules. Therefore, we will locate those molecules and delete those from the orginal sahu datasets** \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the rows that are not read by the mordred.web \n",
    "row_drop = [6,17,18,22,28,40,58,66,116,156,172,174,184,269,270,271,273,277,278]\n",
    "\n",
    "# we can simply use the .drop method to drop the rows.\n",
    "sahu_new_descriptor = sahu.drop(row_drop)\n",
    "\n",
    "# adding the sn so that we can merge the table using same key\n",
    "sahu_new_descriptor.insert(loc=0, column='sn', value=np.arange(len(sahu_new_descriptor)))\n",
    "\n",
    "# let's drop some of the columns because we don't need them!\n",
    "sahu_new_descriptor = sahu_new_descriptor.drop(columns=['#Sno.'])\n",
    "\n",
    "# print(len(sahu_new_descriptor)) # we get the exact 261 molecules now\n",
    "# sahu_new_descriptor.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**At this point, we have downloaded new descriptor and made an indentical file to the original sahu datasets, we may want to merge these two files with index as common merging items**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's merge these two dataframe: we do this using pd.concat([file1,file2],axix=1 or 0, 1 for rows 0 for columns)\n",
    "\n",
    "new_merged_df = pd.merge(sahu_new_descriptor,new_file,on='sn')\n",
    "# new_merged_df.columns\n",
    "# len(new_merged_df)\n",
    "# new_merged_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data are all cleaned and ready to be processed now!**\n",
    "\n",
    "* link to the descriptor: https://mordred-descriptor.github.io/documentation/master/descriptors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.percentile(sahu['PCE'], 25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlonemodel(model, df, target):\n",
    "    train_inputs, train_labels, test_inputs, test_labels = split(df, target)\n",
    "    pred_train, pred_test, evaluation = trainevaluate(model, train_inputs, train_labels, test_inputs, test_labels)\n",
    "    return train_labels, test_labels, pred_train, pred_test, evaluation\n",
    "\n",
    "def split(df, target): #StratifiedShuffleSplit with 4 bins\n",
    "    \n",
    "    bins = [-np.inf, np.percentile(df[target],25), np.percentile(df[target],50), np.percentile(df[target],75), np.inf]\n",
    "    labels = [1, 2, 3, 4]\n",
    "    \n",
    "    df['cat'] = pd.cut(df[target], bins=bins, labels=labels)\n",
    "    df['cat'].hist(bins=20)\n",
    "    #Preparing testing set (20% of instances) and training set using StratifiedShuffleSplit \n",
    "    # --> preserve the distribution!\n",
    "\n",
    "    sss = StratifiedShuffleSplit(n_splits=1, test_size=0.20, random_state=22)\n",
    "    for train_index, test_index in sss.split(df, df['cat']):\n",
    "        strat_train_set = df.loc[train_index]\n",
    "        strat_test_set = df.loc[test_index]\n",
    "\n",
    "    # Drop column 'category' in the dataset    \n",
    "    df = df.drop(columns='cat')\n",
    "    strat_train_set = strat_train_set.drop(columns='cat')\n",
    "    strat_test_set = strat_test_set.drop(columns='cat')\n",
    "    \n",
    "    # Get the location of the target column\n",
    "    target_loc = df.columns.get_loc(target)\n",
    "    input_loc = target_loc + 1\n",
    "    \n",
    "    # Get train_inputs, train_labels, test_inputs, test_labels\n",
    "    train_inputs = strat_train_set.iloc[:, input_loc:]  # inputs\n",
    "    train_labels = strat_train_set.iloc[:, target_loc]  # property that we want to predict\n",
    "\n",
    "    test_inputs = strat_test_set.iloc[:, input_loc:]  # inputs\n",
    "    test_labels = strat_test_set.iloc[:, target_loc]  # property that we want to predict\n",
    "    return train_inputs, train_labels, test_inputs, test_labels\n",
    "\n",
    "def trainevaluate(model, train_inputs, train_labels, test_inputs, test_labels):\n",
    "    model.fit(train_inputs, train_labels)\n",
    "    pred_train = model.predict(train_inputs)\n",
    "    pred_test = model.predict(test_inputs)\n",
    "    \n",
    "    pearson_train = round(pearsonr(train_labels, pred_train)[0], 2)\n",
    "    rmse_train = round(rmse(train_labels, pred_train, squared=False), 2)\n",
    "    mae_train = round(mae(train_labels, pred_train), 2)\n",
    "\n",
    "    pearson_test = round(pearsonr(test_labels, pred_test)[0], 2)\n",
    "    rmse_test = round(rmse(test_labels, pred_test, squared=False), 2)\n",
    "    mae_test = round(mae(test_labels, pred_test), 2)\n",
    "    \n",
    "    # Pay attention to the order of each parameter in this evaluation list\n",
    "    evaluation = [pearson_train, rmse_train, mae_train, pearson_test, rmse_test, mae_test]\n",
    "    \n",
    "    return pred_train, pred_test, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus: predictions with multiple model as once\n",
    "I let you figure out how this function works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mlmultimodel(model, df, target):\n",
    "    pred_train = []\n",
    "    pred_test = []\n",
    "    evaluation = []\n",
    "    train_inputs, train_labels, test_inputs, test_labels = split(df, target)\n",
    "    \n",
    "    for i in range(0, len(model)):\n",
    "        results = trainevaluate(model[i], train_inputs, train_labels, test_inputs, test_labels)\n",
    "        pred_train.append(results[0])\n",
    "        pred_test.append(results[1])\n",
    "        evaluation.append(results[2])\n",
    "    return train_labels, test_labels, pred_train, pred_test, evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plotting\n",
    "I made a function to plot pred_train vs train_labels and pred_test vs test_labels. I explain briefly the codes so that you can make changes to have other functions to plot what you want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotonemodel(prop, pred_train, train_labels, pearson_train, pred_test, test_labels, pearson_test):\n",
    "    plt.figure(figsize=(6,4), dpi=200)\n",
    "    \n",
    "    # Plot trainset as a scatter plot\n",
    "    trainlabel = 'Training set, Pearson r = ' + str(pearson_train)\n",
    "    plt.scatter(pred_train, train_labels, color='red', label=trainlabel)\n",
    "    \n",
    "    # Plot testset as a scatter plot\n",
    "    testlabel = 'Testing set, Pearson r = ' + str(pearson_test)\n",
    "    plt.scatter(pred_test, test_labels, color='blue', label=testlabel)\n",
    "    \n",
    "    # Fining maximum and minimum for both x and y-axis\n",
    "    xymin = round(min(pred_train.min(), train_labels.min(), pred_test.min(), test_labels.min()))\n",
    "    xymax = round(max(pred_train.max(), train_labels.max(), pred_test.max(), test_labels.max()))\n",
    "    \n",
    "    # set min and max for x and y-axis\n",
    "    plt.xlim([xymin, xymax])\n",
    "    plt.ylim([xymin, xymax])\n",
    "    \n",
    "    # Drawing the dashed line y=x on the plot\n",
    "    plt.axline([xymin, xymin], [xymax, xymax], color='black', linestyle='--')\n",
    "    \n",
    "    # Customize your plot\n",
    "    xlabel = 'Experimental ' + prop\n",
    "    ylabel = 'ML predicted ' + prop\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.legend()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checking if these functions work"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if the mlonemodel works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr\n",
    "# plotonemodel('PCE', pred_lr_train, train_labels, evaluation_lr[0], pred_lr_test, test_labels, evaluation_lr[3])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking if the mlmultimodel works"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lr = LinearRegression()\n",
    "# rf = RandomForestRegressor()\n",
    "# ann = MLPRegressor()\n",
    "# # dt = DecisionTreeRegressor()\n",
    "# model = [lr, rf, ann]\n",
    "\n",
    "# train_labels, test_labels, pred_train, pred_test, evaluation = mlmultimodel(model, sahu, 'PCE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# columns = ['pearson_train', 'rmse_train', 'mae_train', 'pearson_test', 'rmse_test', 'mae_test']\n",
    "# index = ['lr', 'rf', 'ann']\n",
    "\n",
    "# eval_df = pd.DataFrame(evaluation, index=index, columns=columns)\n",
    "# eval_df\n",
    "# sahu.feature_importance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluation[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot for linear regresstion model\n",
    "# plotonemodel('PCE', pred_train[0], train_labels, evaluation[0][0], pred_test[0], test_labels, evaluation[0][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot for ann model\n",
    "# plotonemodel('PCE', pred_train[2], train_labels, evaluation[2][0], pred_test[2], test_labels, evaluation[2][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rf\n",
    "# plotonemodel('PCE', pred_train[1], train_labels, evaluation[1][0], pred_test[1], test_labels, evaluation[1][3])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dt\n",
    "# plotonemodel('PCE', pred_train[3], train_labels, evaluation[3][0], pred_test[3], test_labels, evaluation[3][3])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "nav_menu": {
   "height": "279px",
   "width": "309px"
  },
  "toc": {
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": "block",
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
